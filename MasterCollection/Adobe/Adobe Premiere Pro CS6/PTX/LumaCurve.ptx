	.version 2.3
	.target sm_20
	.address_size 64
	// compiled with C:\mightykilt64/shared/adobe/MediaCore//External/3rdParty/NVIDIA/CUDA/win/bin/../open64/lib//be.exe
	// nvopencc 4.0 built on 2011-05-13

	.visible .func (.param .f32 __cudaretf__Z31InterpolateNaturalCubicSpline2DPKfS0_S0_yf) _Z31InterpolateNaturalCubicSpline2DPKfS0_S0_yf (.param .u64 __cudaparmf1__Z31InterpolateNaturalCubicSpline2DPKfS0_S0_yf, .param .u64 __cudaparmf2__Z31InterpolateNaturalCubicSpline2DPKfS0_S0_yf, .param .u64 __cudaparmf3__Z31InterpolateNaturalCubicSpline2DPKfS0_S0_yf, .param .u64 __cudaparmf4__Z31InterpolateNaturalCubicSpline2DPKfS0_S0_yf, .param .f32 __cudaparmf5__Z31InterpolateNaturalCubicSpline2DPKfS0_S0_yf)

	//-----------------------------------------------------------
	// Compiling C:/Users/dvaeng/AppData/Local/Temp/tmpxft_000037a8_00000000-11_LumaCurve.cpp3.i (C:/Users/dvaeng/AppData/Local/Temp/ccBI#.a13700)
	//-----------------------------------------------------------

	//-----------------------------------------------------------
	// Options:
	//-----------------------------------------------------------
	//  Target:ptx, ISA:sm_20, Endian:little, Pointer Size:64
	//  -O3	(Optimization level)
	//  -g0	(Debug level)
	//  -m2	(Report advisories)
	//-----------------------------------------------------------

	.file	1	"C:/Users/dvaeng/AppData/Local/Temp/tmpxft_000037a8_00000000-10_LumaCurve.cudafe2.gpu"
	.file	2	"C:\mightykilt64\shared\adobe\MediaCore\GPUFoundation\API\Inc\GPUFoundation/KernelSupport/DevicePixelFormat.h"
	.file	3	"C:\mightykilt64\shared\adobe\MediaCore\Renderers\RendererGPU\Inc\Effects/ColorCorrector_Curves.h"
	.file	4	"C:\mightykilt64\shared\adobe\MediaCore\GPUFoundation\API\Inc\GPUFoundation/KernelSupport/PixelRGB.h"
	.file	5	"C:\mightykilt64\shared\adobe\MediaCore\GPUFoundation\API\Inc\GPUFoundation/KernelSupport/PixelYUV.h"
	.file	6	"c:\program files (x86)\microsoft visual studio 10.0\vc\include\codeanalysis\sourceannotations.h"
	.file	7	"C:\mightykilt64\shared\adobe\MediaCore\External\3rdParty\NVIDIA\CUDA\win\include\crt/device_runtime.h"
	.file	8	"C:\mightykilt64\shared\adobe\MediaCore\External\3rdParty\NVIDIA\CUDA\win\include\host_defines.h"
	.file	9	"C:\mightykilt64\shared\adobe\MediaCore\External\3rdParty\NVIDIA\CUDA\win\include\builtin_types.h"
	.file	10	"c:\mightykilt64\shared\adobe\mediacore\external\3rdparty\nvidia\cuda\win\include\device_types.h"
	.file	11	"c:\mightykilt64\shared\adobe\mediacore\external\3rdparty\nvidia\cuda\win\include\driver_types.h"
	.file	12	"c:\mightykilt64\shared\adobe\mediacore\external\3rdparty\nvidia\cuda\win\include\surface_types.h"
	.file	13	"c:\mightykilt64\shared\adobe\mediacore\external\3rdparty\nvidia\cuda\win\include\texture_types.h"
	.file	14	"c:\mightykilt64\shared\adobe\mediacore\external\3rdparty\nvidia\cuda\win\include\vector_types.h"
	.file	15	"c:\mightykilt64\shared\adobe\mediacore\external\3rdparty\nvidia\cuda\win\include\builtin_types.h"
	.file	16	"c:\mightykilt64\shared\adobe\mediacore\external\3rdparty\nvidia\cuda\win\include\host_defines.h"
	.file	17	"C:\mightykilt64\shared\adobe\MediaCore\External\3rdParty\NVIDIA\CUDA\win\include\device_launch_parameters.h"
	.file	18	"c:\mightykilt64\shared\adobe\mediacore\external\3rdparty\nvidia\cuda\win\include\crt\storage_class.h"
	.file	19	"C:\Program Files (x86)\Microsoft Visual Studio 10.0\VC\INCLUDE\time.h"
	.file	20	"C:/mightykilt64/shared/adobe/MediaCore/Renderers/RendererGPU/Src/Effects/LumaCurve.cu"
	.file	21	"C:\mightykilt64\shared\adobe\MediaCore\External\3rdParty\NVIDIA\CUDA\win\include\common_functions.h"
	.file	22	"c:\mightykilt64\shared\adobe\mediacore\external\3rdparty\nvidia\cuda\win\include\math_functions.h"
	.file	23	"c:\mightykilt64\shared\adobe\mediacore\external\3rdparty\nvidia\cuda\win\include\math_constants.h"
	.file	24	"c:\mightykilt64\shared\adobe\mediacore\external\3rdparty\nvidia\cuda\win\include\device_functions.h"
	.file	25	"c:\mightykilt64\shared\adobe\mediacore\external\3rdparty\nvidia\cuda\win\include\sm_11_atomic_functions.h"
	.file	26	"c:\mightykilt64\shared\adobe\mediacore\external\3rdparty\nvidia\cuda\win\include\sm_12_atomic_functions.h"
	.file	27	"c:\mightykilt64\shared\adobe\mediacore\external\3rdparty\nvidia\cuda\win\include\sm_13_double_functions.h"
	.file	28	"c:\mightykilt64\shared\adobe\mediacore\external\3rdparty\nvidia\cuda\win\include\sm_20_atomic_functions.h"
	.file	29	"c:\mightykilt64\shared\adobe\mediacore\external\3rdparty\nvidia\cuda\win\include\sm_20_intrinsics.h"
	.file	30	"c:\mightykilt64\shared\adobe\mediacore\external\3rdparty\nvidia\cuda\win\include\surface_functions.h"
	.file	31	"c:\mightykilt64\shared\adobe\mediacore\external\3rdparty\nvidia\cuda\win\include\texture_fetch_functions.h"
	.file	32	"c:\mightykilt64\shared\adobe\mediacore\external\3rdparty\nvidia\cuda\win\include\math_functions_dbl_ptx3.h"
	.file	33	"C:\mightykilt64\shared\adobe\MediaCore\GPUFoundation\API\Inc\GPUFoundation/KernelSupport/ColorSpaceConvert.h"


	.visible .func (.param .f32 __cudaretf__Z31InterpolateNaturalCubicSpline2DPKfS0_S0_yf) _Z31InterpolateNaturalCubicSpline2DPKfS0_S0_yf (.param .u64 __cudaparmf1__Z31InterpolateNaturalCubicSpline2DPKfS0_S0_yf, .param .u64 __cudaparmf2__Z31InterpolateNaturalCubicSpline2DPKfS0_S0_yf, .param .u64 __cudaparmf3__Z31InterpolateNaturalCubicSpline2DPKfS0_S0_yf, .param .u64 __cudaparmf4__Z31InterpolateNaturalCubicSpline2DPKfS0_S0_yf, .param .f32 __cudaparmf5__Z31InterpolateNaturalCubicSpline2DPKfS0_S0_yf)
	{
	.reg .u64 %rd<28>;
	.reg .f32 %f<33>;
	.reg .pred %p<6>;
	.loc	3	42	0
$LDWbegin__Z31InterpolateNaturalCubicSpline2DPKfS0_S0_yf:
	ld.param.u64 	%rd1, [__cudaparmf1__Z31InterpolateNaturalCubicSpline2DPKfS0_S0_yf];
	mov.s64 	%rd2, %rd1;
	ld.param.u64 	%rd3, [__cudaparmf2__Z31InterpolateNaturalCubicSpline2DPKfS0_S0_yf];
	mov.s64 	%rd4, %rd3;
	ld.param.u64 	%rd5, [__cudaparmf3__Z31InterpolateNaturalCubicSpline2DPKfS0_S0_yf];
	mov.s64 	%rd6, %rd5;
	ld.param.u64 	%rd7, [__cudaparmf4__Z31InterpolateNaturalCubicSpline2DPKfS0_S0_yf];
	mov.s64 	%rd8, %rd7;
	ld.param.f32 	%f1, [__cudaparmf5__Z31InterpolateNaturalCubicSpline2DPKfS0_S0_yf];
	mov.f32 	%f2, %f1;
	mov.f32 	%f3, %f2;
	sub.u64 	%rd9, %rd8, 1;
	mov.s64 	%rd10, %rd9;
	mov.u64 	%rd11, 1;
	setp.le.u64 	%p1, %rd9, %rd11;
	@%p1 bra 	$Lt_0_4866;
	mov.u64 	%rd12, 0;
$Lt_0_3330:
	add.u64 	%rd13, %rd10, %rd12;
	shr.u64 	%rd14, %rd13, 1;
	mul.lo.u64 	%rd15, %rd14, 4;
	add.u64 	%rd16, %rd2, %rd15;
	ld.f32 	%f4, [%rd16+0];
	setp.gt.ftz.f32 	%p2, %f4, %f2;
	@!%p2 bra 	$Lt_0_3842;
	mov.s64 	%rd10, %rd14;
	bra.uni 	$Lt_0_3586;
$Lt_0_3842:
	mov.s64 	%rd12, %rd14;
$Lt_0_3586:
	sub.u64 	%rd17, %rd10, %rd12;
	mov.u64 	%rd18, 1;
	setp.gt.u64 	%p3, %rd17, %rd18;
	@%p3 bra 	$Lt_0_3330;
	bra.uni 	$Lt_0_2818;
$Lt_0_4866:
	mov.u64 	%rd12, 0;
$Lt_0_2818:
	mul.lo.u64 	%rd19, %rd10, 4;
	mul.lo.u64 	%rd20, %rd12, 4;
	add.u64 	%rd21, %rd19, %rd2;
	add.u64 	%rd22, %rd20, %rd2;
	ld.f32 	%f5, [%rd21+0];
	ld.f32 	%f6, [%rd22+0];
	sub.ftz.f32 	%f7, %f5, %f6;
	mov.f32 	%f8, 0f00000000;     	// 0
	setp.neu.ftz.f32 	%p4, %f7, %f8;
	@!%p4 bra 	$Lt_0_4354;
	sub.ftz.f32 	%f9, %f5, %f2;
	sub.ftz.f32 	%f10, %f2, %f6;
	div.approx.ftz.f32 	%f11, %f10, %f7;
	div.approx.ftz.f32 	%f12, %f9, %f7;
	add.u64 	%rd23, %rd19, %rd6;
	ld.f32 	%f13, [%rd23+0];
	mul.ftz.f32 	%f14, %f11, %f11;
	mul.ftz.f32 	%f15, %f11, %f14;
	sub.ftz.f32 	%f16, %f15, %f11;
	mul.ftz.f32 	%f17, %f13, %f16;
	add.u64 	%rd24, %rd20, %rd6;
	ld.f32 	%f18, [%rd24+0];
	mul.ftz.f32 	%f19, %f12, %f12;
	mul.ftz.f32 	%f20, %f12, %f19;
	sub.ftz.f32 	%f21, %f20, %f12;
	fma.rn.ftz.f32 	%f22, %f18, %f21, %f17;
	mul.ftz.f32 	%f23, %f7, %f7;
	mul.ftz.f32 	%f24, %f22, %f23;
	mov.f32 	%f25, 0f40c00000;    	// 6
	div.approx.ftz.f32 	%f26, %f24, %f25;
	add.u64 	%rd25, %rd19, %rd4;
	ld.f32 	%f27, [%rd25+0];
	mul.ftz.f32 	%f28, %f27, %f11;
	add.u64 	%rd26, %rd20, %rd4;
	ld.f32 	%f29, [%rd26+0];
	fma.rn.ftz.f32 	%f30, %f29, %f12, %f28;
	add.ftz.f32 	%f3, %f26, %f30;
$Lt_0_4354:
	mov.f32 	%f31, %f3;
	st.param.f32 	[__cudaretf__Z31InterpolateNaturalCubicSpline2DPKfS0_S0_yf], %f31;
	ret;
$LDWend__Z31InterpolateNaturalCubicSpline2DPKfS0_S0_yf:
	} // _Z31InterpolateNaturalCubicSpline2DPKfS0_S0_yf

	.entry LumaCurve_MaskKernel (
		.param .u64 __cudaparm_LumaCurve_MaskKernel_inImage,
		.param .s32 __cudaparm_LumaCurve_MaskKernel_inPitch,
		.param .u64 __cudaparm_LumaCurve_MaskKernel_inSecondaryMask,
		.param .s32 __cudaparm_LumaCurve_MaskKernel_inSecondaryPitch,
		.param .u32 __cudaparm_LumaCurve_MaskKernel_inDeviceFormat,
		.param .s32 __cudaparm_LumaCurve_MaskKernel_inWidth,
		.param .s32 __cudaparm_LumaCurve_MaskKernel_inHeight)
	{
	.reg .u32 %r<31>;
	.reg .u64 %rd<13>;
	.reg .f32 %f<5>;
	.reg .pred %p<5>;
	.loc	20	42	0
$LDWbegin_LumaCurve_MaskKernel:
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mul.lo.u32 	%r3, %r1, %r2;
	mov.u32 	%r4, %ctaid.y;
	mov.u32 	%r5, %ntid.y;
	mul.lo.u32 	%r6, %r4, %r5;
	mov.u32 	%r7, %tid.x;
	add.u32 	%r8, %r7, %r3;
	mov.u32 	%r9, %tid.y;
	add.u32 	%r10, %r9, %r6;
	ld.param.s32 	%r11, [__cudaparm_LumaCurve_MaskKernel_inWidth];
	set.gt.u32.s32 	%r12, %r11, %r8;
	neg.s32 	%r13, %r12;
	ld.param.s32 	%r14, [__cudaparm_LumaCurve_MaskKernel_inHeight];
	set.gt.u32.s32 	%r15, %r14, %r10;
	neg.s32 	%r16, %r15;
	and.b32 	%r17, %r13, %r16;
	mov.u32 	%r18, 0;
	setp.eq.s32 	%p1, %r17, %r18;
	@%p1 bra 	$Lt_1_3842;
	ld.param.u64 	%rd1, [__cudaparm_LumaCurve_MaskKernel_inSecondaryMask];
	mov.u64 	%rd2, 0;
	setp.eq.u64 	%p2, %rd1, %rd2;
	@%p2 bra 	$Lt_1_4610;
	ld.param.s32 	%r19, [__cudaparm_LumaCurve_MaskKernel_inSecondaryPitch];
	mul.lo.s32 	%r20, %r19, %r10;
	add.s32 	%r21, %r8, %r20;
	cvt.s64.s32 	%rd3, %r21;
	mul.wide.s32 	%rd4, %r21, 4;
	add.u64 	%rd5, %rd1, %rd4;
	ld.global.f32 	%f1, [%rd5+0];
	bra.uni 	$Lt_1_4354;
$Lt_1_4610:
	mov.f32 	%f1, 0f3f800000;     	// 1
$Lt_1_4354:
	ld.param.s32 	%r22, [__cudaparm_LumaCurve_MaskKernel_inPitch];
	mul.lo.s32 	%r23, %r22, %r10;
	add.s32 	%r24, %r8, %r23;
	cvt.s64.s32 	%rd6, %r24;
	ld.param.u64 	%rd7, [__cudaparm_LumaCurve_MaskKernel_inImage];
	ld.param.u32 	%r25, [__cudaparm_LumaCurve_MaskKernel_inDeviceFormat];
	mov.u32 	%r26, 0;
	setp.ne.s32 	%p3, %r25, %r26;
	@%p3 bra 	$Lt_1_5122;
	{ .reg .b32 %b1;
	cvt.rn.ftz.f16.f32	%b1, %f1;
	mov.b32		%r27, %b1; }
	mov.s32 	%r28, %r27;
	mul.lo.u64 	%rd8, %rd6, 8;
	add.u64 	%rd9, %rd7, %rd8;
	mov.f32 	%f2, 0f3f800000;     	// 1
	{ .reg .b32 %b1;
	cvt.rn.ftz.f16.f32	%b1, %f2;
	mov.b32		%r29, %b1; }
	st.global.v4.u16 	[%rd9+0], {%r28,%r28,%r28,%r29};
	bra.uni 	$Lt_1_4866;
$Lt_1_5122:
	mul.lo.u64 	%rd10, %rd6, 16;
	add.u64 	%rd11, %rd7, %rd10;
	mov.f32 	%f3, 0f3f800000;     	// 1
	st.global.v4.f32 	[%rd11+0], {%f1,%f1,%f1,%f3};
$Lt_1_4866:
$Lt_1_3842:
	exit;
$LDWend_LumaCurve_MaskKernel:
	} // LumaCurve_MaskKernel
	.global .align 8 .b8 inSpline[200];

	.entry LumaCurve_LumaKernel (
		.param .u64 __cudaparm_LumaCurve_LumaKernel_inImage,
		.param .s32 __cudaparm_LumaCurve_LumaKernel_inPitch,
		.param .u64 __cudaparm_LumaCurve_LumaKernel_inSecondaryMask,
		.param .s32 __cudaparm_LumaCurve_LumaKernel_inSecondaryPitch,
		.param .u32 __cudaparm_LumaCurve_LumaKernel_inDeviceFormat,
		.param .s32 __cudaparm_LumaCurve_LumaKernel_inWidth,
		.param .s32 __cudaparm_LumaCurve_LumaKernel_inHeight,
		.param .u64 __cudaparm_LumaCurve_LumaKernel_ConstantinSpline)
	{
	.reg .u32 %r<34>;
	.reg .u64 %rd<36>;
	.reg .f32 %f<52>;
	.reg .pred %p<12>;
$LDWbegin_LumaCurve_LumaKernel:
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mul.lo.u32 	%r3, %r1, %r2;
	mov.u32 	%r4, %ctaid.y;
	mov.u32 	%r5, %ntid.y;
	mul.lo.u32 	%r6, %r4, %r5;
	mov.u32 	%r7, %tid.x;
	add.u32 	%r8, %r7, %r3;
	mov.u32 	%r9, %tid.y;
	add.u32 	%r10, %r9, %r6;
	ld.param.s32 	%r11, [__cudaparm_LumaCurve_LumaKernel_inWidth];
	set.gt.u32.s32 	%r12, %r11, %r8;
	neg.s32 	%r13, %r12;
	ld.param.s32 	%r14, [__cudaparm_LumaCurve_LumaKernel_inHeight];
	set.gt.u32.s32 	%r15, %r14, %r10;
	neg.s32 	%r16, %r15;
	and.b32 	%r17, %r13, %r16;
	mov.u32 	%r18, 0;
	setp.eq.s32 	%p1, %r17, %r18;
	@%p1 bra 	$Lt_2_13570;
	ld.param.u32 	%r19, [__cudaparm_LumaCurve_LumaKernel_inDeviceFormat];
	mov.s32 	%r20, 0;
	setp.eq.s32 	%p2, %r19, %r20;
	ld.param.s32 	%r21, [__cudaparm_LumaCurve_LumaKernel_inPitch];
	mul.lo.s32 	%r22, %r21, %r10;
	add.s32 	%r23, %r8, %r22;
	cvt.s64.s32 	%rd1, %r23;
	ld.param.u64 	%rd2, [__cudaparm_LumaCurve_LumaKernel_inImage];
	@!%p2 bra 	$Lt_2_9218;
	mul.lo.u64 	%rd3, %rd1, 8;
	add.u64 	%rd4, %rd2, %rd3;
	ld.global.v4.u16 	{%r24,%r25,%r26,_}, [%rd4+0];
	{ .reg .b32 %b1;
	mov.b32		%b1, %r24;
	cvt.ftz.f32.f16	%f1, %b1; }
	{ .reg .b32 %b1;
	mov.b32		%b1, %r25;
	cvt.ftz.f32.f16	%f2, %b1; }
	{ .reg .b32 %b1;
	mov.b32		%b1, %r26;
	cvt.ftz.f32.f16	%f3, %b1; }
	bra.uni 	$Lt_2_8962;
$Lt_2_9218:
	mul.lo.u64 	%rd5, %rd1, 16;
	add.u64 	%rd6, %rd2, %rd5;
	ld.global.v4.f32 	{%f1,%f2,%f3,_}, [%rd6+0];
$Lt_2_8962:
	ld.param.u64 	%rd7, [__cudaparm_LumaCurve_LumaKernel_inSecondaryMask];
	mov.u64 	%rd8, 0;
	setp.eq.u64 	%p3, %rd7, %rd8;
	@%p3 bra 	$Lt_2_9730;
	ld.param.s32 	%r27, [__cudaparm_LumaCurve_LumaKernel_inSecondaryPitch];
	mul.lo.s32 	%r28, %r27, %r10;
	add.s32 	%r29, %r8, %r28;
	cvt.s64.s32 	%rd9, %r29;
	mul.wide.s32 	%rd10, %r29, 4;
	add.u64 	%rd11, %rd7, %rd10;
	ld.global.f32 	%f4, [%rd11+0];
	bra.uni 	$Lt_2_9474;
$Lt_2_9730:
	mov.f32 	%f4, 0f3f800000;     	// 1
$Lt_2_9474:
	mov.f32 	%f5, 0f3f1645a2;     	// 0.587
	mul.ftz.f32 	%f6, %f2, %f5;
	mov.f32 	%f7, 0f3e991687;     	// 0.299
	fma.rn.ftz.f32 	%f8, %f7, %f3, %f6;
	mov.f32 	%f9, 0f3de978d5;     	// 0.114
	fma.rn.ftz.f32 	%f10, %f9, %f1, %f8;
	mov.f32 	%f11, %f10;
	mov.f32 	%f12, 0f3a83126f;    	// 0.001
	setp.gt.ftz.f32 	%p4, %f4, %f12;
	@!%p4 bra 	$Lt_2_9986;
	mov.f32 	%f13, 0f00000000;    	// 0
	setp.lt.ftz.f32 	%p5, %f10, %f13;
	@!%p5 bra 	$Lt_2_10754;
	ld.global.f32 	%f14, [inSpline+64];
	add.ftz.f32 	%f15, %f14, %f10;
	bra.uni 	$Lt_2_11010;
$Lt_2_10754:
	ld.global.u64 	%rd12, [inSpline+192];
	mov.f32 	%f16, 0f3f800000;    	// 1
	setp.gt.ftz.f32 	%p6, %f10, %f16;
	@!%p6 bra 	$Lt_2_11266;
	mov.f32 	%f17, 0f3f800000;    	// 1
	mov.u64 	%rd13, inSpline;
	mul.lo.u64 	%rd14, %rd12, 4;
	add.u64 	%rd15, %rd13, %rd14;
	ld.global.f32 	%f18, [%rd15+60];
	sub.ftz.f32 	%f19, %f17, %f18;
	sub.ftz.f32 	%f15, %f10, %f19;
	bra.uni 	$Lt_2_11010;
$Lt_2_11266:
	.loc	3	42	0
	mov.f32 	%f20, %f10;
	sub.u64 	%rd16, %rd12, 1;
	mov.s64 	%rd17, %rd16;
	mov.u64 	%rd18, 1;
	setp.le.u64 	%p7, %rd16, %rd18;
	@%p7 bra 	$Lt_2_14082;
	mov.u64 	%rd19, 0;
	mov.u64 	%rd20, inSpline;
$Lt_2_12034:
	add.u64 	%rd21, %rd17, %rd19;
	shr.u64 	%rd22, %rd21, 1;
	mul.lo.u64 	%rd23, %rd22, 4;
	add.u64 	%rd24, %rd20, %rd23;
	ld.global.f32 	%f21, [%rd24+0];
	setp.gt.ftz.f32 	%p8, %f21, %f10;
	@!%p8 bra 	$Lt_2_12546;
	mov.s64 	%rd17, %rd22;
	bra.uni 	$Lt_2_12290;
$Lt_2_12546:
	mov.s64 	%rd19, %rd22;
$Lt_2_12290:
	sub.u64 	%rd25, %rd17, %rd19;
	mov.u64 	%rd26, 1;
	setp.gt.u64 	%p9, %rd25, %rd26;
	@%p9 bra 	$Lt_2_12034;
	bra.uni 	$Lt_2_11522;
$Lt_2_14082:
	mov.u64 	%rd19, 0;
	mov.u64 	%rd20, inSpline;
$Lt_2_11522:
	mul.lo.u64 	%rd27, %rd17, 4;
	add.u64 	%rd28, %rd20, %rd27;
	mul.lo.u64 	%rd29, %rd19, 4;
	add.u64 	%rd30, %rd20, %rd29;
	ld.global.f32 	%f22, [%rd28+0];
	ld.global.f32 	%f23, [%rd30+0];
	sub.ftz.f32 	%f24, %f22, %f23;
	mov.f32 	%f25, 0f00000000;    	// 0
	setp.neu.ftz.f32 	%p10, %f24, %f25;
	@!%p10 bra 	$Lt_2_13058;
	.loc	24	544	0
	sub.ftz.f32 	%f26, %f22, %f10;
	sub.ftz.f32 	%f27, %f10, %f23;
	div.approx.ftz.f32 	%f28, %f26, %f24;
	div.approx.ftz.f32 	%f29, %f27, %f24;
	ld.global.f32 	%f30, [%rd28+128];
	mul.ftz.f32 	%f31, %f29, %f29;
	mul.ftz.f32 	%f32, %f29, %f31;
	sub.ftz.f32 	%f33, %f32, %f29;
	mul.ftz.f32 	%f34, %f30, %f33;
	ld.global.f32 	%f35, [%rd30+128];
	mul.ftz.f32 	%f36, %f28, %f28;
	mul.ftz.f32 	%f37, %f28, %f36;
	sub.ftz.f32 	%f38, %f37, %f28;
	fma.rn.ftz.f32 	%f39, %f35, %f38, %f34;
	mul.ftz.f32 	%f40, %f24, %f24;
	mul.ftz.f32 	%f41, %f39, %f40;
	mov.f32 	%f42, 0f40c00000;    	// 6
	div.approx.ftz.f32 	%f43, %f41, %f42;
	.loc	3	42	0
	ld.global.f32 	%f44, [%rd28+64];
	mul.ftz.f32 	%f45, %f44, %f29;
	ld.global.f32 	%f46, [%rd30+64];
	fma.rn.ftz.f32 	%f47, %f46, %f28, %f45;
	add.ftz.f32 	%f20, %f43, %f47;
$Lt_2_13058:
	.loc	20	42	0
	mov.f32 	%f15, %f20;
$Lt_2_11010:
$Lt_2_10498:
	sub.ftz.f32 	%f48, %f15, %f10;
	fma.rn.ftz.f32 	%f11, %f4, %f48, %f10;
$Lt_2_9986:
	@!%p2 bra 	$Lt_2_13826;
	{ .reg .b32 %b1;
	cvt.rn.ftz.f16.f32	%b1, %f11;
	mov.b32		%r30, %b1; }
	mov.s32 	%r31, %r30;
	mul.lo.u64 	%rd31, %rd1, 8;
	add.u64 	%rd32, %rd2, %rd31;
	mov.f32 	%f49, 0f3f800000;    	// 1
	{ .reg .b32 %b1;
	cvt.rn.ftz.f16.f32	%b1, %f49;
	mov.b32		%r32, %b1; }
	st.global.v4.u16 	[%rd32+0], {%r31,%r31,%r31,%r32};
	bra.uni 	$Lt_2_13570;
$Lt_2_13826:
	mul.lo.u64 	%rd33, %rd1, 16;
	add.u64 	%rd34, %rd2, %rd33;
	mov.f32 	%f50, 0f3f800000;    	// 1
	st.global.v4.f32 	[%rd34+0], {%f11,%f11,%f11,%f50};
$Lt_2_13570:
$Lt_2_8450:
	exit;
$LDWend_LumaCurve_LumaKernel:
	} // LumaCurve_LumaKernel

	.entry LumaCurve_CompositeKernel (
		.param .u64 __cudaparm_LumaCurve_CompositeKernel_inImage,
		.param .s32 __cudaparm_LumaCurve_CompositeKernel_inPitch,
		.param .u64 __cudaparm_LumaCurve_CompositeKernel_inSecondaryMask,
		.param .s32 __cudaparm_LumaCurve_CompositeKernel_inSecondaryPitch,
		.param .u32 __cudaparm_LumaCurve_CompositeKernel_inDeviceFormat,
		.param .s32 __cudaparm_LumaCurve_CompositeKernel_inWidth,
		.param .s32 __cudaparm_LumaCurve_CompositeKernel_inHeight,
		.param .u64 __cudaparm_LumaCurve_CompositeKernel_ConstantinSpline)
	{
	.reg .u32 %r<36>;
	.reg .u64 %rd<36>;
	.reg .f32 %f<71>;
	.reg .pred %p<12>;
$LDWbegin_LumaCurve_CompositeKernel:
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mul.lo.u32 	%r3, %r1, %r2;
	mov.u32 	%r4, %ctaid.y;
	mov.u32 	%r5, %ntid.y;
	mul.lo.u32 	%r6, %r4, %r5;
	mov.u32 	%r7, %tid.x;
	add.u32 	%r8, %r7, %r3;
	mov.u32 	%r9, %tid.y;
	add.u32 	%r10, %r9, %r6;
	ld.param.s32 	%r11, [__cudaparm_LumaCurve_CompositeKernel_inWidth];
	set.gt.u32.s32 	%r12, %r11, %r8;
	neg.s32 	%r13, %r12;
	ld.param.s32 	%r14, [__cudaparm_LumaCurve_CompositeKernel_inHeight];
	set.gt.u32.s32 	%r15, %r14, %r10;
	neg.s32 	%r16, %r15;
	and.b32 	%r17, %r13, %r16;
	mov.u32 	%r18, 0;
	setp.eq.s32 	%p1, %r17, %r18;
	@%p1 bra 	$Lt_3_13570;
	ld.param.u32 	%r19, [__cudaparm_LumaCurve_CompositeKernel_inDeviceFormat];
	mov.s32 	%r20, 0;
	setp.eq.s32 	%p2, %r19, %r20;
	ld.param.s32 	%r21, [__cudaparm_LumaCurve_CompositeKernel_inPitch];
	mul.lo.s32 	%r22, %r21, %r10;
	add.s32 	%r23, %r8, %r22;
	cvt.s64.s32 	%rd1, %r23;
	ld.param.u64 	%rd2, [__cudaparm_LumaCurve_CompositeKernel_inImage];
	@!%p2 bra 	$Lt_3_9218;
	mul.lo.u64 	%rd3, %rd1, 8;
	add.u64 	%rd4, %rd2, %rd3;
	ld.global.v4.u16 	{%r24,%r25,%r26,%r27}, [%rd4+0];
	{ .reg .b32 %b1;
	mov.b32		%b1, %r24;
	cvt.ftz.f32.f16	%f1, %b1; }
	{ .reg .b32 %b1;
	mov.b32		%b1, %r25;
	cvt.ftz.f32.f16	%f2, %b1; }
	{ .reg .b32 %b1;
	mov.b32		%b1, %r26;
	cvt.ftz.f32.f16	%f3, %b1; }
	{ .reg .b32 %b1;
	mov.b32		%b1, %r27;
	cvt.ftz.f32.f16	%f4, %b1; }
	bra.uni 	$Lt_3_8962;
$Lt_3_9218:
	mul.lo.u64 	%rd5, %rd1, 16;
	add.u64 	%rd6, %rd2, %rd5;
	ld.global.v4.f32 	{%f1,%f2,%f3,%f4}, [%rd6+0];
$Lt_3_8962:
	ld.param.u64 	%rd7, [__cudaparm_LumaCurve_CompositeKernel_inSecondaryMask];
	mov.u64 	%rd8, 0;
	setp.eq.u64 	%p3, %rd7, %rd8;
	@%p3 bra 	$Lt_3_9730;
	ld.param.s32 	%r28, [__cudaparm_LumaCurve_CompositeKernel_inSecondaryPitch];
	mul.lo.s32 	%r29, %r28, %r10;
	add.s32 	%r30, %r8, %r29;
	cvt.s64.s32 	%rd9, %r30;
	mul.wide.s32 	%rd10, %r30, 4;
	add.u64 	%rd11, %rd7, %rd10;
	ld.global.f32 	%f5, [%rd11+0];
	bra.uni 	$Lt_3_9474;
$Lt_3_9730:
	mov.f32 	%f5, 0f3f800000;     	// 1
$Lt_3_9474:
	mov.f32 	%f6, 0f3f1645a2;     	// 0.587
	mul.ftz.f32 	%f7, %f2, %f6;
	mov.f32 	%f8, 0f3e991687;     	// 0.299
	fma.rn.ftz.f32 	%f9, %f8, %f3, %f7;
	mov.f32 	%f10, 0f3de978d5;    	// 0.114
	fma.rn.ftz.f32 	%f11, %f10, %f1, %f9;
	mov.f32 	%f12, %f11;
	mov.f32 	%f13, 0f3a83126f;    	// 0.001
	setp.gt.ftz.f32 	%p4, %f5, %f13;
	@!%p4 bra 	$Lt_3_9986;
	mov.f32 	%f14, 0f00000000;    	// 0
	setp.lt.ftz.f32 	%p5, %f11, %f14;
	@!%p5 bra 	$Lt_3_10754;
	ld.global.f32 	%f15, [inSpline+64];
	add.ftz.f32 	%f16, %f15, %f11;
	bra.uni 	$Lt_3_11010;
$Lt_3_10754:
	ld.global.u64 	%rd12, [inSpline+192];
	mov.f32 	%f17, 0f3f800000;    	// 1
	setp.gt.ftz.f32 	%p6, %f11, %f17;
	@!%p6 bra 	$Lt_3_11266;
	mov.f32 	%f18, 0f3f800000;    	// 1
	mov.u64 	%rd13, inSpline;
	mul.lo.u64 	%rd14, %rd12, 4;
	add.u64 	%rd15, %rd13, %rd14;
	ld.global.f32 	%f19, [%rd15+60];
	sub.ftz.f32 	%f20, %f18, %f19;
	sub.ftz.f32 	%f16, %f11, %f20;
	bra.uni 	$Lt_3_11010;
$Lt_3_11266:
	.loc	3	42	0
	mov.f32 	%f21, %f11;
	sub.u64 	%rd16, %rd12, 1;
	mov.s64 	%rd17, %rd16;
	mov.u64 	%rd18, 1;
	setp.le.u64 	%p7, %rd16, %rd18;
	@%p7 bra 	$Lt_3_14082;
	mov.u64 	%rd19, 0;
	mov.u64 	%rd20, inSpline;
$Lt_3_12034:
	add.u64 	%rd21, %rd17, %rd19;
	shr.u64 	%rd22, %rd21, 1;
	mul.lo.u64 	%rd23, %rd22, 4;
	add.u64 	%rd24, %rd20, %rd23;
	ld.global.f32 	%f22, [%rd24+0];
	setp.gt.ftz.f32 	%p8, %f22, %f11;
	@!%p8 bra 	$Lt_3_12546;
	mov.s64 	%rd17, %rd22;
	bra.uni 	$Lt_3_12290;
$Lt_3_12546:
	mov.s64 	%rd19, %rd22;
$Lt_3_12290:
	sub.u64 	%rd25, %rd17, %rd19;
	mov.u64 	%rd26, 1;
	setp.gt.u64 	%p9, %rd25, %rd26;
	@%p9 bra 	$Lt_3_12034;
	bra.uni 	$Lt_3_11522;
$Lt_3_14082:
	mov.u64 	%rd19, 0;
	mov.u64 	%rd20, inSpline;
$Lt_3_11522:
	mul.lo.u64 	%rd27, %rd17, 4;
	add.u64 	%rd28, %rd20, %rd27;
	mul.lo.u64 	%rd29, %rd19, 4;
	add.u64 	%rd30, %rd20, %rd29;
	ld.global.f32 	%f23, [%rd28+0];
	ld.global.f32 	%f24, [%rd30+0];
	sub.ftz.f32 	%f25, %f23, %f24;
	mov.f32 	%f26, 0f00000000;    	// 0
	setp.neu.ftz.f32 	%p10, %f25, %f26;
	@!%p10 bra 	$Lt_3_13058;
	.loc	24	544	0
	sub.ftz.f32 	%f27, %f23, %f11;
	sub.ftz.f32 	%f28, %f11, %f24;
	div.approx.ftz.f32 	%f29, %f27, %f25;
	div.approx.ftz.f32 	%f30, %f28, %f25;
	ld.global.f32 	%f31, [%rd28+128];
	mul.ftz.f32 	%f32, %f30, %f30;
	mul.ftz.f32 	%f33, %f30, %f32;
	sub.ftz.f32 	%f34, %f33, %f30;
	mul.ftz.f32 	%f35, %f31, %f34;
	ld.global.f32 	%f36, [%rd30+128];
	mul.ftz.f32 	%f37, %f29, %f29;
	mul.ftz.f32 	%f38, %f29, %f37;
	sub.ftz.f32 	%f39, %f38, %f29;
	fma.rn.ftz.f32 	%f40, %f36, %f39, %f35;
	mul.ftz.f32 	%f41, %f25, %f25;
	mul.ftz.f32 	%f42, %f40, %f41;
	mov.f32 	%f43, 0f40c00000;    	// 6
	div.approx.ftz.f32 	%f44, %f42, %f43;
	.loc	3	42	0
	ld.global.f32 	%f45, [%rd28+64];
	mul.ftz.f32 	%f46, %f45, %f30;
	ld.global.f32 	%f47, [%rd30+64];
	fma.rn.ftz.f32 	%f48, %f47, %f29, %f46;
	add.ftz.f32 	%f21, %f44, %f48;
$Lt_3_13058:
	.loc	20	42	0
	mov.f32 	%f16, %f21;
$Lt_3_11010:
$Lt_3_10498:
	sub.ftz.f32 	%f49, %f16, %f11;
	fma.rn.ftz.f32 	%f12, %f5, %f49, %f11;
$Lt_3_9986:
	mov.f32 	%f50, 0fbea99b6f;    	// -0.331264
	mul.ftz.f32 	%f51, %f2, %f50;
	mov.f32 	%f52, 0fbed65e46;    	// -0.418688
	mul.ftz.f32 	%f53, %f2, %f52;
	mov.f32 	%f54, 0fbe2cc921;    	// -0.168736
	fma.rn.ftz.f32 	%f55, %f54, %f3, %f51;
	mov.f32 	%f56, 0f3f000000;    	// 0.5
	fma.rn.ftz.f32 	%f57, %f56, %f3, %f53;
	mov.f32 	%f58, 0f3f000000;    	// 0.5
	fma.rn.ftz.f32 	%f59, %f58, %f1, %f55;
	mov.f32 	%f60, 0fbda686e8;    	// -0.081312
	fma.rn.ftz.f32 	%f61, %f60, %f1, %f57;
	mov.f32 	%f62, 0f3fe2d0e5;    	// 1.772
	fma.rn.ftz.f32 	%f63, %f62, %f59, %f12;
	mov.f32 	%f64, 0fbeb03298;    	// -0.344136
	fma.rn.ftz.f32 	%f65, %f64, %f59, %f12;
	mov.f32 	%f66, 0f3fb374bc;    	// 1.402
	fma.rn.ftz.f32 	%f67, %f66, %f61, %f12;
	mov.f32 	%f68, 0fbf36d19e;    	// -0.714136
	fma.rn.ftz.f32 	%f69, %f68, %f61, %f65;
	@!%p2 bra 	$Lt_3_13826;
	mul.lo.u64 	%rd31, %rd1, 8;
	add.u64 	%rd32, %rd2, %rd31;
	{ .reg .b32 %b1;
	cvt.rn.ftz.f16.f32	%b1, %f63;
	mov.b32		%r31, %b1; }
	{ .reg .b32 %b1;
	cvt.rn.ftz.f16.f32	%b1, %f69;
	mov.b32		%r32, %b1; }
	{ .reg .b32 %b1;
	cvt.rn.ftz.f16.f32	%b1, %f67;
	mov.b32		%r33, %b1; }
	{ .reg .b32 %b1;
	cvt.rn.ftz.f16.f32	%b1, %f4;
	mov.b32		%r34, %b1; }
	st.global.v4.u16 	[%rd32+0], {%r31,%r32,%r33,%r34};
	bra.uni 	$Lt_3_13570;
$Lt_3_13826:
	mul.lo.u64 	%rd33, %rd1, 16;
	add.u64 	%rd34, %rd2, %rd33;
	st.global.v4.f32 	[%rd34+0], {%f63,%f69,%f67,%f4};
$Lt_3_13570:
$Lt_3_8450:
	exit;
$LDWend_LumaCurve_CompositeKernel:
	} // LumaCurve_CompositeKernel
	.global .align 4 .b8 kRGB32f_To_601YPbPr[36] = {135,22,153,62,162,69,22,63,213,120,233,61,33,201,44,190,111,155,169,190,0,0,0,63,0,0,0,63,70,94,214,190,232,134,166,189};
	.global .align 4 .b8 k601YPbPr_To_RGB32f[36] = {0,0,128,63,0,0,0,0,188,116,179,63,0,0,128,63,152,50,176,190,158,209,54,191,0,0,128,63,229,208,226,63,0,0,0,0};
	.const .align 4 .b8 kRGB32f_To_601YCbCr[36] = {70,246,130,66,145,141,0,67,94,186,199,65,33,48,23,194,240,103,148,194,0,0,224,66,0,0,224,66,111,146,187,194,70,182,145,193};
	.const .align 4 .b8 k601YCbCr_To_RGB32f[36] = {37,160,149,59,0,0,0,0,182,23,205,59,37,160,149,59,40,15,201,186,156,239,80,187,37,160,149,59,236,155,1,60,0,0,0,0};
	.const .align 4 .b8 kRGB8u_To_601YCbCr[36] = {219,121,131,62,152,14,1,63,18,131,200,61,174,199,23,190,238,252,148,190,197,224,224,62,197,224,224,62,217,78,188,190,174,71,146,189};
	.const .align 4 .b8 k601YCbCr_To_RGB8u[36] = {127,10,149,63,0,0,0,0,160,74,204,63,127,10,149,63,254,148,200,190,184,30,80,191,127,10,149,63,78,26,1,64,0,0,0,0};
	.const .align 4 .b8 kRGB8u_To_601YCbCrFullRange[36] = {135,22,153,62,162,69,22,63,213,120,233,61,166,27,44,190,39,241,168,190,250,254,254,62,250,254,254,62,43,135,213,190,59,223,165,189};
	.const .align 4 .b8 k601YCbCrFullRange_To_RGB8u[36] = {0,0,128,63,0,0,0,0,72,193,178,63,0,0,128,63,143,130,175,190,225,26,54,191,0,0,128,63,20,238,225,63,0,0,0,0};
	.const .align 4 .b8 kRGB32f_To_601YCbCrFullRange[36] = {113,125,152,66,92,175,21,67,92,143,232,65,158,111,43,194,49,72,168,194,0,0,254,66,0,0,254,66,170,177,212,194,88,57,165,193};
	.const .align 4 .b8 k601YCbCrFullRange_To_RGB32f[36] = {129,128,128,59,0,0,0,0,188,116,179,59,129,128,128,59,194,50,176,186,179,209,54,187,129,128,128,59,229,208,226,59,0,0,0,0};
	.const .align 4 .b8 kRGB32f_To_709YPbPr[36] = {208,179,89,62,89,23,55,63,152,221,147,61,186,164,234,189,210,86,197,190,0,0,0,63,0,0,0,63,190,134,232,190,16,202,59,189};
	.const .align 4 .b8 k709YPbPr_To_RGB32f[36] = {0,0,128,63,0,0,0,0,12,147,201,63,0,0,128,63,221,209,63,190,243,173,239,190,0,0,128,63,77,132,237,63,0,0,0,0};
	.const .align 4 .b8 kRGB32f_To_709YCbCr[36] = {106,60,58,66,6,161,28,67,244,253,124,65,223,79,205,193,8,172,172,194,0,0,224,66,0,0,224,66,195,117,203,194,236,81,36,193};
	.const .align 4 .b8 k709YCbCr_To_RGB32f[36] = {37,160,149,59,0,0,0,0,239,94,230,59,37,160,149,59,33,57,91,186,178,245,8,187,37,160,149,59,82,185,7,60,0,0,0,0};
	.const .align 4 .b8 k709YCbCrFullRange_To_RGB32f[36] = {131,128,128,59,0,0,0,0,28,147,201,59,131,128,128,59,61,210,63,186,248,173,239,186,131,128,128,59,82,132,237,59,0,0,0,0};
	.const .align 4 .b8 kRGB8u_To_709YCbCr[36] = {207,247,58,62,53,62,29,63,231,251,125,61,147,24,206,61,23,89,173,190,197,224,224,62,197,224,224,62,12,66,204,190,195,245,36,189};
	.const .align 4 .b8 k709YCbCr_To_RGB8u[36] = {127,10,149,63,0,0,0,0,147,120,229,63,127,10,149,63,53,94,90,190,205,108,8,191,127,10,149,63,154,49,7,64,0,0,0,0};
	.const .align 4 .b8 k709YCbCr_To_601YCbCr[36] = {0,0,128,63,23,100,203,61,1,77,68,62,0,0,0,0,18,103,125,63,10,158,226,189,0,0,0,0,61,98,148,189,249,191,123,63};
	.const .align 4 .b8 k601YCbCr_To_709YCbCr[36] = {0,0,128,63,122,165,236,189,179,237,84,190,0,0,0,0,204,98,130,63,216,188,234,61,0,0,0,0,74,179,153,61,234,61,131,63};
	.const .align 4 .b8 kYCbCrOffset[12] = {0,0,128,65,0,0,0,67,0,0,0,67};
	.const .align 4 .b8 kYCbCrFullRangeOffset[12] = {0,0,0,0,0,0,0,67,0,0,0,67};

